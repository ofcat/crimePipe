{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Engineering Semester Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin setting our data consumer, we need to make sure we have a zookeper service running. When working with Apache Kafka, ZooKeeper is primarily used to track the status of nodes in the Kafka cluster and maintain a list of Kafka topics and messages.\n",
    "\n",
    "In our case, we are using an image of zookeper that is running on a Docker container. To run the container, simply navigate to the folder containig docker-compose file of zookeper and execute command 'docker-compose up -d' to start the process.\n",
    "\n",
    "Another tool that was used in our project is Offset Explorer, an open-source application that allows to connect to Kafka Cluster and visually see all the topics and messages. \n",
    "\n",
    "* Offset Exploer can dowloaded via this link - https://www.kafkatool.com/download.html\n",
    "\n",
    "Additionaly, don't forget to install PySpark and Spark itslef both of which are vital to run this application. \n",
    "\n",
    "* Apache Spark can be installed from here - https://spark.apache.org/downloads.html.\n",
    "* PySpark can be installed via pip command - 'pip install pyspark'\n",
    "* Kafka can also be installed via pip command - 'pip install kafka-python'\n",
    "\n",
    "On top of that, don't forget to set up Environment variables for Spark in Path of your machine\n",
    "\n",
    "*  SPARK_HOME\n",
    "* JAVA_HOME\n",
    "* HADOOP_HOME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create new Spark instance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Spark Streaming with Kafka we need to initialise spark variable with a Kafka connector. Keep in mind that version string (\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\") should be changed according to what version of spark is used on your machine.\n",
    "\n",
    "Spark version can be checked via this command - '!pyspark --version'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/vasilii/apache-spark/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/vasilii/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/vasilii/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6c4d9ac0-bee9-4c9f-98ad-470a5e771a8f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 602ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from local-m2-cache in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6c4d9ac0-bee9-4c9f-98ad-470a5e771a8f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/48ms)\n",
      "22/06/29 11:57:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# attention: the .config line is specific for the aida-n2\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName('CrimeData')\n",
    "         # Add kafka package (so that spark can find kafka for streaming)\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create stream dataframe setting kafka server, topic and offset option**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyspark --version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now connecting to the previosly created Kafka topic where Kafka producer had writen our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stream dataframe setting kafka server, topic and offset option\n",
    "df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:29092\") # kafka server\n",
    "  .option(\"subscribe\", \"crimeData-chicago_100k\") # topic\n",
    "  .option(\"startingOffsets\", \"earliest\") # start from beginning \n",
    "  .option(\"multiline\", True)\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Key and Value pairs to String type and make it available as a dataframe again**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting key and value paris from Kafka topic from byte to String allows us to view the data in String format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert binary to string key and value\n",
    "df1 = (df\n",
    "    .withColumn(\"key\", df[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df[\"value\"].cast(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a structure that is used as schema for the streamed data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Streaming required streamed data to have a Schema, which is just a textual description of how columns look inside value column of Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, ArrayType, StructField, BooleanType, LongType, IntegerType, DoubleType, DateType, TimestampType\n",
    "\n",
    "# Event data schema\n",
    "schema_wiki = StructType(\n",
    "    [StructField(\"$schema\",StringType(),True),\n",
    "     StructField(\"id\",StringType(),True),\n",
    "     StructField(\"case_number\",StringType(),True),\n",
    "     StructField(\"date\",TimestampType(),True),\n",
    "     StructField(\"block\",StringType(),True),\n",
    "     StructField(\"iucr\",StringType(),True),\n",
    "     StructField(\"primary_type\",StringType(),True),\n",
    "     StructField(\"description\",StringType(),True),\n",
    "     StructField(\"location_description\",StringType(),True),\n",
    "     StructField(\"arrest\",StringType(),True),\n",
    "     StructField(\"domestic\",StringType(),True),\n",
    "     StructField(\"beat\",StringType(),True),\n",
    "     StructField(\"district\",StringType(),True),\n",
    "     StructField(\"ward\",StringType(),True),\n",
    "     StructField(\"community_area\",StringType(),True),\n",
    "     StructField(\"x_coordinate\",StringType(),True),\n",
    "     StructField(\"y_coordinate\",StringType(),True),\n",
    "     StructField(\"updated_on\",StringType(),True),\n",
    "     StructField(\"latitude\",StringType(),True),\n",
    "     StructField(\"longitude\",StringType(),True),\n",
    "     #StructField(\"latitude\",StringType(),True),\n",
    "     StructField(\"location\", StructType(\n",
    "                                    [StructField(\"latitude\",StringType(),True),\n",
    "                                    StructField(\"longitude\",StringType(),True),\n",
    "                                    StructField(\"human_address\",StringType(),True),]),True)\n",
    "     \n",
    "    \n",
    "    ]\n",
    "     \n",
    "      )\n",
    "\n",
    "schema = ArrayType(schema_wiki, True)\n",
    "\n",
    "# Create dataframe setting schema for event data\n",
    "df_wiki = (df1\n",
    "           # Sets schema for event data\n",
    "           .withColumn(\"value\", from_json(\"value\", schema_wiki))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(df_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start streaming data from Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/29 11:58:08 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/vr/48qd0dvd3zx5h01r32l1_5k40000gn/T/temporary-b93a4a3b-9b98-4bc7-aaa0-c29e74b8a804. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/06/29 11:58:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# Create query stream with memory sink\n",
    "queryStream = (df_wiki\n",
    " .writeStream\n",
    " .format(\"memory\")\n",
    " .queryName(\"wiki_changes\")\n",
    " .outputMode(\"update\")\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:3f4a5669-d3d5-49f8-8dae-9ebae4487290 | NAME:wiki_changes\n"
     ]
    }
   ],
   "source": [
    "# Check active streams\n",
    "for s in spark.streams.active:\n",
    "    print(\"ID:{} | NAME:{}\".format(s.id, s.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make queries to the streamed data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using spark.sql() command, we are able to query data, we have an option to store the output as a spark or pandas dataframe. We are doing both to showcase this functionality.\n",
    "\n",
    "While using spar.sql() on our data, we have to add prefix 'value.' to every column we would like to query, this is because our data is stored withing the value column of the Kafka Topic. You can see from df2 dataframe output that 'crimeData_Chicago_100k' topic has additional columns such as: key, value, topic, partition, offset and timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- case_number: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- block: string (nullable = true)\n",
      " |-- iucr: string (nullable = true)\n",
      " |-- primary_type: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location_description: string (nullable = true)\n",
      " |-- arrest: string (nullable = true)\n",
      " |-- domestic: string (nullable = true)\n",
      " |-- beat: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- ward: string (nullable = true)\n",
      " |-- community_area: string (nullable = true)\n",
      " |-- x_coordinate: string (nullable = true)\n",
      " |-- y_coordinate: string (nullable = true)\n",
      " |-- updated_on: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- latitude: string (nullable = true)\n",
      " |    |-- longitude: string (nullable = true)\n",
      " |    |-- human_address: string (nullable = true)\n",
      "\n",
      "DataFrame[id: string, case_number: string, date: timestamp, block: string, iucr: string, primary_type: string, description: string, location_description: string, arrest: string, domestic: string, beat: string, district: string, ward: string, community_area: string, x_coordinate: string, y_coordinate: string, updated_on: string, latitude: string, longitude: string, location: struct<latitude:string,longitude:string,human_address:string>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/29 11:58:37 WARN TaskSetManager: Stage 1 contains a task of very large size (15094 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_number</th>\n",
       "      <th>date</th>\n",
       "      <th>block</th>\n",
       "      <th>iucr</th>\n",
       "      <th>primary_type</th>\n",
       "      <th>description</th>\n",
       "      <th>location_description</th>\n",
       "      <th>arrest</th>\n",
       "      <th>domestic</th>\n",
       "      <th>beat</th>\n",
       "      <th>district</th>\n",
       "      <th>ward</th>\n",
       "      <th>community_area</th>\n",
       "      <th>x_coordinate</th>\n",
       "      <th>y_coordinate</th>\n",
       "      <th>updated_on</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12736693</td>\n",
       "      <td>JF287903</td>\n",
       "      <td>2022-06-20 23:56:00</td>\n",
       "      <td>007XX W GARFIELD BLVD</td>\n",
       "      <td>0486</td>\n",
       "      <td>BATTERY</td>\n",
       "      <td>DOMESTIC BATTERY SIMPLE</td>\n",
       "      <td>APARTMENT</td>\n",
       "      <td>false</td>\n",
       "      <td>true</td>\n",
       "      <td>0711</td>\n",
       "      <td>007</td>\n",
       "      <td>20</td>\n",
       "      <td>68</td>\n",
       "      <td>1172439</td>\n",
       "      <td>1868272</td>\n",
       "      <td>2022-06-27T16:49:20.000</td>\n",
       "      <td>41.793979431</td>\n",
       "      <td>-87.643208858</td>\n",
       "      <td>(41.793979431, -87.643208858, {\"address\": \"\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12736637</td>\n",
       "      <td>JF287926</td>\n",
       "      <td>2022-06-20 23:56:00</td>\n",
       "      <td>015XX S KARLOV AVE</td>\n",
       "      <td>143A</td>\n",
       "      <td>WEAPONS VIOLATION</td>\n",
       "      <td>UNLAWFUL POSSESSION - HANDGUN</td>\n",
       "      <td>STREET</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>1012</td>\n",
       "      <td>010</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>1149303</td>\n",
       "      <td>1892129</td>\n",
       "      <td>2022-06-27T16:49:20.000</td>\n",
       "      <td>41.859924664</td>\n",
       "      <td>-87.727431051</td>\n",
       "      <td>(41.859924664, -87.727431051, {\"address\": \"\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12736641</td>\n",
       "      <td>JF287904</td>\n",
       "      <td>2022-06-20 23:54:00</td>\n",
       "      <td>005XX N OGDEN AVE</td>\n",
       "      <td>051A</td>\n",
       "      <td>ASSAULT</td>\n",
       "      <td>AGGRAVATED - HANDGUN</td>\n",
       "      <td>STREET</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>1214</td>\n",
       "      <td>012</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>1167891</td>\n",
       "      <td>1903682</td>\n",
       "      <td>2022-06-27T16:49:20.000</td>\n",
       "      <td>41.891246762</td>\n",
       "      <td>-87.658866407</td>\n",
       "      <td>(41.891246762, -87.658866407, {\"address\": \"\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12736591</td>\n",
       "      <td>JF287899</td>\n",
       "      <td>2022-06-20 23:54:00</td>\n",
       "      <td>034XX S ARCHER AVE</td>\n",
       "      <td>1345</td>\n",
       "      <td>CRIMINAL DAMAGE</td>\n",
       "      <td>TO CITY OF CHICAGO PROPERTY</td>\n",
       "      <td>STREET</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>0912</td>\n",
       "      <td>009</td>\n",
       "      <td>12</td>\n",
       "      <td>59</td>\n",
       "      <td>1163091</td>\n",
       "      <td>1881589</td>\n",
       "      <td>2022-06-27T16:49:20.000</td>\n",
       "      <td>41.830723665</td>\n",
       "      <td>-87.677114617</td>\n",
       "      <td>(41.830723665, -87.677114617, {\"address\": \"\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12736636</td>\n",
       "      <td>JF287914</td>\n",
       "      <td>2022-06-20 23:52:00</td>\n",
       "      <td>006XX W 103RD ST</td>\n",
       "      <td>1320</td>\n",
       "      <td>CRIMINAL DAMAGE</td>\n",
       "      <td>TO VEHICLE</td>\n",
       "      <td>STREET</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>2232</td>\n",
       "      <td>022</td>\n",
       "      <td>9</td>\n",
       "      <td>73</td>\n",
       "      <td>1173886</td>\n",
       "      <td>1836576</td>\n",
       "      <td>2022-06-27T16:49:20.000</td>\n",
       "      <td>41.706969624</td>\n",
       "      <td>-87.638840966</td>\n",
       "      <td>(41.706969624, -87.638840966, {\"address\": \"\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>12579526</td>\n",
       "      <td>JE491744</td>\n",
       "      <td>2021-12-29 10:11:00</td>\n",
       "      <td>047XX W ADAMS ST</td>\n",
       "      <td>1477</td>\n",
       "      <td>WEAPONS VIOLATION</td>\n",
       "      <td>RECKLESS FIREARM DISCHARGE</td>\n",
       "      <td>ALLEY</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>1113</td>\n",
       "      <td>011</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>1144798</td>\n",
       "      <td>1898706</td>\n",
       "      <td>2022-01-05T15:49:10.000</td>\n",
       "      <td>41.878058829</td>\n",
       "      <td>-87.743802049</td>\n",
       "      <td>(41.878058829, -87.743802049, {\"address\": \"\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>12579529</td>\n",
       "      <td>JE491750</td>\n",
       "      <td>2021-12-29 10:09:00</td>\n",
       "      <td>123XX S YALE AVE</td>\n",
       "      <td>1477</td>\n",
       "      <td>WEAPONS VIOLATION</td>\n",
       "      <td>RECKLESS FIREARM DISCHARGE</td>\n",
       "      <td>ALLEY</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>0523</td>\n",
       "      <td>005</td>\n",
       "      <td>34</td>\n",
       "      <td>53</td>\n",
       "      <td>1176840</td>\n",
       "      <td>1823134</td>\n",
       "      <td>2022-01-05T15:49:10.000</td>\n",
       "      <td>41.670016883</td>\n",
       "      <td>-87.62842629</td>\n",
       "      <td>(41.670016883, -87.62842629, {\"address\": \"\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>12579610</td>\n",
       "      <td>JE491722</td>\n",
       "      <td>2021-12-29 10:08:00</td>\n",
       "      <td>005XX N MICHIGAN AVE</td>\n",
       "      <td>0460</td>\n",
       "      <td>BATTERY</td>\n",
       "      <td>SIMPLE</td>\n",
       "      <td>DEPARTMENT STORE</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>1834</td>\n",
       "      <td>018</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>1177342</td>\n",
       "      <td>1903838</td>\n",
       "      <td>2022-01-05T15:49:10.000</td>\n",
       "      <td>41.891465732</td>\n",
       "      <td>-87.624153044</td>\n",
       "      <td>(41.891465732, -87.624153044, {\"address\": \"\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>12579538</td>\n",
       "      <td>JE491797</td>\n",
       "      <td>2021-12-29 10:00:00</td>\n",
       "      <td>056XX W NORTH AVE</td>\n",
       "      <td>1320</td>\n",
       "      <td>CRIMINAL DAMAGE</td>\n",
       "      <td>TO VEHICLE</td>\n",
       "      <td>RESIDENCE</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>2531</td>\n",
       "      <td>025</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>1138253</td>\n",
       "      <td>1910065</td>\n",
       "      <td>2022-01-05T15:49:10.000</td>\n",
       "      <td>41.9093502</td>\n",
       "      <td>-87.76755907</td>\n",
       "      <td>(41.9093502, -87.76755907, {\"address\": \"\", \"ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>12580798</td>\n",
       "      <td>JE492923</td>\n",
       "      <td>2021-12-29 10:00:00</td>\n",
       "      <td>020XX E 80TH ST</td>\n",
       "      <td>1310</td>\n",
       "      <td>CRIMINAL DAMAGE</td>\n",
       "      <td>TO PROPERTY</td>\n",
       "      <td>RESIDENCE</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>0414</td>\n",
       "      <td>004</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>1191352</td>\n",
       "      <td>1852335</td>\n",
       "      <td>2022-01-05T15:49:10.000</td>\n",
       "      <td>41.749809517</td>\n",
       "      <td>-87.574372642</td>\n",
       "      <td>(41.749809517, -87.574372642, {\"address\": \"\", ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id case_number                date                  block  iucr  \\\n",
       "0      12736693    JF287903 2022-06-20 23:56:00  007XX W GARFIELD BLVD  0486   \n",
       "1      12736637    JF287926 2022-06-20 23:56:00     015XX S KARLOV AVE  143A   \n",
       "2      12736641    JF287904 2022-06-20 23:54:00      005XX N OGDEN AVE  051A   \n",
       "3      12736591    JF287899 2022-06-20 23:54:00     034XX S ARCHER AVE  1345   \n",
       "4      12736636    JF287914 2022-06-20 23:52:00       006XX W 103RD ST  1320   \n",
       "...         ...         ...                 ...                    ...   ...   \n",
       "99995  12579526    JE491744 2021-12-29 10:11:00       047XX W ADAMS ST  1477   \n",
       "99996  12579529    JE491750 2021-12-29 10:09:00       123XX S YALE AVE  1477   \n",
       "99997  12579610    JE491722 2021-12-29 10:08:00   005XX N MICHIGAN AVE  0460   \n",
       "99998  12579538    JE491797 2021-12-29 10:00:00      056XX W NORTH AVE  1320   \n",
       "99999  12580798    JE492923 2021-12-29 10:00:00        020XX E 80TH ST  1310   \n",
       "\n",
       "            primary_type                    description location_description  \\\n",
       "0                BATTERY        DOMESTIC BATTERY SIMPLE            APARTMENT   \n",
       "1      WEAPONS VIOLATION  UNLAWFUL POSSESSION - HANDGUN               STREET   \n",
       "2                ASSAULT           AGGRAVATED - HANDGUN               STREET   \n",
       "3        CRIMINAL DAMAGE    TO CITY OF CHICAGO PROPERTY               STREET   \n",
       "4        CRIMINAL DAMAGE                     TO VEHICLE               STREET   \n",
       "...                  ...                            ...                  ...   \n",
       "99995  WEAPONS VIOLATION     RECKLESS FIREARM DISCHARGE                ALLEY   \n",
       "99996  WEAPONS VIOLATION     RECKLESS FIREARM DISCHARGE                ALLEY   \n",
       "99997            BATTERY                         SIMPLE     DEPARTMENT STORE   \n",
       "99998    CRIMINAL DAMAGE                     TO VEHICLE            RESIDENCE   \n",
       "99999    CRIMINAL DAMAGE                    TO PROPERTY            RESIDENCE   \n",
       "\n",
       "      arrest domestic  beat district ward community_area x_coordinate  \\\n",
       "0      false     true  0711      007   20             68      1172439   \n",
       "1       true    false  1012      010   24             29      1149303   \n",
       "2       true    false  1214      012   27             24      1167891   \n",
       "3      false    false  0912      009   12             59      1163091   \n",
       "4      false    false  2232      022    9             73      1173886   \n",
       "...      ...      ...   ...      ...  ...            ...          ...   \n",
       "99995   true    false  1113      011   28             25      1144798   \n",
       "99996  false    false  0523      005   34             53      1176840   \n",
       "99997   true    false  1834      018   42              8      1177342   \n",
       "99998  false    false  2531      025   29             25      1138253   \n",
       "99999  false    false  0414      004    8             46      1191352   \n",
       "\n",
       "      y_coordinate               updated_on      latitude      longitude  \\\n",
       "0          1868272  2022-06-27T16:49:20.000  41.793979431  -87.643208858   \n",
       "1          1892129  2022-06-27T16:49:20.000  41.859924664  -87.727431051   \n",
       "2          1903682  2022-06-27T16:49:20.000  41.891246762  -87.658866407   \n",
       "3          1881589  2022-06-27T16:49:20.000  41.830723665  -87.677114617   \n",
       "4          1836576  2022-06-27T16:49:20.000  41.706969624  -87.638840966   \n",
       "...            ...                      ...           ...            ...   \n",
       "99995      1898706  2022-01-05T15:49:10.000  41.878058829  -87.743802049   \n",
       "99996      1823134  2022-01-05T15:49:10.000  41.670016883   -87.62842629   \n",
       "99997      1903838  2022-01-05T15:49:10.000  41.891465732  -87.624153044   \n",
       "99998      1910065  2022-01-05T15:49:10.000    41.9093502   -87.76755907   \n",
       "99999      1852335  2022-01-05T15:49:10.000  41.749809517  -87.574372642   \n",
       "\n",
       "                                                location  \n",
       "0      (41.793979431, -87.643208858, {\"address\": \"\", ...  \n",
       "1      (41.859924664, -87.727431051, {\"address\": \"\", ...  \n",
       "2      (41.891246762, -87.658866407, {\"address\": \"\", ...  \n",
       "3      (41.830723665, -87.677114617, {\"address\": \"\", ...  \n",
       "4      (41.706969624, -87.638840966, {\"address\": \"\", ...  \n",
       "...                                                  ...  \n",
       "99995  (41.878058829, -87.743802049, {\"address\": \"\", ...  \n",
       "99996  (41.670016883, -87.62842629, {\"address\": \"\", \"...  \n",
       "99997  (41.891465732, -87.624153044, {\"address\": \"\", ...  \n",
       "99998  (41.9093502, -87.76755907, {\"address\": \"\", \"ci...  \n",
       "99999  (41.749809517, -87.574372642, {\"address\": \"\", ...  \n",
       "\n",
       "[100000 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>topic</th>\n",
       "      <th>partition</th>\n",
       "      <th>offset</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timestampType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>(None, 12736693, JF287903, 2022-06-20 23:56:00...</td>\n",
       "      <td>crimeData-chicago_100k</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-06-28 18:58:07.956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>(None, 12736637, JF287926, 2022-06-20 23:56:00...</td>\n",
       "      <td>crimeData-chicago_100k</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-28 18:58:07.956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>(None, 12736641, JF287904, 2022-06-20 23:54:00...</td>\n",
       "      <td>crimeData-chicago_100k</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-06-28 18:58:07.956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>(None, 12736591, JF287899, 2022-06-20 23:54:00...</td>\n",
       "      <td>crimeData-chicago_100k</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-06-28 18:58:07.956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>(None, 12736636, JF287914, 2022-06-20 23:52:00...</td>\n",
       "      <td>crimeData-chicago_100k</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-06-28 18:58:07.957</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>None</td>\n",
       "      <td>(None, 12579526, JE491744, 2021-12-29 10:11:00...</td>\n",
       "      <td>crimeData-chicago_100k</td>\n",
       "      <td>0</td>\n",
       "      <td>99995</td>\n",
       "      <td>2022-06-28 18:58:56.703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>None</td>\n",
       "      <td>(None, 12579529, JE491750, 2021-12-29 10:09:00...</td>\n",
       "      <td>crimeData-chicago_100k</td>\n",
       "      <td>0</td>\n",
       "      <td>99996</td>\n",
       "      <td>2022-06-28 18:58:56.703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>None</td>\n",
       "      <td>(None, 12579610, JE491722, 2021-12-29 10:08:00...</td>\n",
       "      <td>crimeData-chicago_100k</td>\n",
       "      <td>0</td>\n",
       "      <td>99997</td>\n",
       "      <td>2022-06-28 18:58:56.703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>None</td>\n",
       "      <td>(None, 12579538, JE491797, 2021-12-29 10:00:00...</td>\n",
       "      <td>crimeData-chicago_100k</td>\n",
       "      <td>0</td>\n",
       "      <td>99998</td>\n",
       "      <td>2022-06-28 18:58:56.703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>None</td>\n",
       "      <td>(None, 12580798, JE492923, 2021-12-29 10:00:00...</td>\n",
       "      <td>crimeData-chicago_100k</td>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>2022-06-28 18:58:56.703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        key                                              value  \\\n",
       "0      None  (None, 12736693, JF287903, 2022-06-20 23:56:00...   \n",
       "1      None  (None, 12736637, JF287926, 2022-06-20 23:56:00...   \n",
       "2      None  (None, 12736641, JF287904, 2022-06-20 23:54:00...   \n",
       "3      None  (None, 12736591, JF287899, 2022-06-20 23:54:00...   \n",
       "4      None  (None, 12736636, JF287914, 2022-06-20 23:52:00...   \n",
       "...     ...                                                ...   \n",
       "99995  None  (None, 12579526, JE491744, 2021-12-29 10:11:00...   \n",
       "99996  None  (None, 12579529, JE491750, 2021-12-29 10:09:00...   \n",
       "99997  None  (None, 12579610, JE491722, 2021-12-29 10:08:00...   \n",
       "99998  None  (None, 12579538, JE491797, 2021-12-29 10:00:00...   \n",
       "99999  None  (None, 12580798, JE492923, 2021-12-29 10:00:00...   \n",
       "\n",
       "                        topic  partition  offset               timestamp  \\\n",
       "0      crimeData-chicago_100k          0       0 2022-06-28 18:58:07.956   \n",
       "1      crimeData-chicago_100k          0       1 2022-06-28 18:58:07.956   \n",
       "2      crimeData-chicago_100k          0       2 2022-06-28 18:58:07.956   \n",
       "3      crimeData-chicago_100k          0       3 2022-06-28 18:58:07.956   \n",
       "4      crimeData-chicago_100k          0       4 2022-06-28 18:58:07.957   \n",
       "...                       ...        ...     ...                     ...   \n",
       "99995  crimeData-chicago_100k          0   99995 2022-06-28 18:58:56.703   \n",
       "99996  crimeData-chicago_100k          0   99996 2022-06-28 18:58:56.703   \n",
       "99997  crimeData-chicago_100k          0   99997 2022-06-28 18:58:56.703   \n",
       "99998  crimeData-chicago_100k          0   99998 2022-06-28 18:58:56.703   \n",
       "99999  crimeData-chicago_100k          0   99999 2022-06-28 18:58:56.703   \n",
       "\n",
       "       timestampType  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "...              ...  \n",
       "99995              0  \n",
       "99996              0  \n",
       "99997              0  \n",
       "99998              0  \n",
       "99999              0  \n",
       "\n",
       "[100000 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process interrupted.\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "matplotlib.rc('font', family='DejaVu Sans')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "try:\n",
    "    i=1\n",
    "    while True:\n",
    "        # Clear output\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "        df = spark.sql(\n",
    "                \"\"\"\n",
    "                    select\n",
    "                       value.id,\n",
    "                       value.case_number,\n",
    "                       value.date,\n",
    "                       value.block,\n",
    "                       value.iucr,\n",
    "                       value.primary_type,\n",
    "                       value.description,\n",
    "                       value.location_description,\n",
    "                       value.arrest,\n",
    "                       value.domestic,\n",
    "                       value.beat,\n",
    "                       value.district,\n",
    "                       value.ward,\n",
    "                       value.community_area,\n",
    "                       value.x_coordinate,\n",
    "                       value.y_coordinate,\n",
    "                       value.updated_on,\n",
    "                       value.latitude,\n",
    "                       value.longitude,\n",
    "                       value.location\n",
    "                    from\n",
    "                        wiki_changes\n",
    "                    \n",
    "                \"\"\"\n",
    "        )#.toPandas()\n",
    "        \n",
    "        df.printSchema()\n",
    "        print(df)\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "        df1 = spark.sql(\n",
    "                \"\"\"\n",
    "                    select\n",
    "                       value.id,\n",
    "                       value.case_number,\n",
    "                       value.date,\n",
    "                       value.block,\n",
    "                       value.iucr,\n",
    "                       value.primary_type,\n",
    "                       value.description,\n",
    "                       value.location_description,\n",
    "                       value.arrest,\n",
    "                       value.domestic,\n",
    "                       value.beat,\n",
    "                       value.district,\n",
    "                       value.ward,\n",
    "                       value.community_area,\n",
    "                       value.x_coordinate,\n",
    "                       value.y_coordinate,\n",
    "                       value.updated_on,\n",
    "                       value.latitude,\n",
    "                       value.longitude,\n",
    "                       value.location\n",
    "                       \n",
    "                    from\n",
    "                        wiki_changes\n",
    "                    \n",
    "                \"\"\"\n",
    "        ).toPandas()\n",
    "        \n",
    "        display(df1)\n",
    "        \n",
    "        df2 = spark.sql(\"select * from wiki_changes\").toPandas()\n",
    "        \n",
    "        display(df2)\n",
    "\n",
    "        \n",
    "        sleep(10)\n",
    "        i=i+1\n",
    "except KeyboardInterrupt:\n",
    "    print(\"process interrupted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop stream\n",
    "queryStream.stop()\n",
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double checking that all the data has been streamed. It is supposed to be 100.000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enabling spark.sql.repl.eagerEval.enabled configuration for the eager evaluation of PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting basic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use both Spark Dataframe and Spark SQL tools to query the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the most popular crime type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using columns \"primary_type\" as well counting all rows to have a total number of crimes grouped by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/29 12:12:40 WARN TaskSetManager: Stage 23 contains a task of very large size (15094 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|        primary_type|count|\n",
      "+--------------------+-----+\n",
      "|OTHER NARCOTIC VI...|    1|\n",
      "|        NON-CRIMINAL|    2|\n",
      "|    PUBLIC INDECENCY|    2|\n",
      "|            GAMBLING|    5|\n",
      "|   HUMAN TRAFFICKING|   10|\n",
      "|           OBSCENITY|   14|\n",
      "|          KIDNAPPING|   43|\n",
      "|        INTIMIDATION|   64|\n",
      "|LIQUOR LAW VIOLATION|   90|\n",
      "|CONCEALED CARRY L...|   91|\n",
      "|        PROSTITUTION|  138|\n",
      "|            STALKING|  171|\n",
      "|               ARSON|  177|\n",
      "|INTERFERENCE WITH...|  188|\n",
      "|            HOMICIDE|  301|\n",
      "|PUBLIC PEACE VIOL...|  323|\n",
      "|         SEX OFFENSE|  555|\n",
      "|CRIMINAL SEXUAL A...|  684|\n",
      "|OFFENSE INVOLVING...|  913|\n",
      "|   CRIMINAL TRESPASS| 1913|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crimeByType = df.groupby(\"primary_type\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/29 12:13:01 WARN TaskSetManager: Stage 26 contains a task of very large size (15094 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|        primary_type|count(1)|\n",
      "+--------------------+--------+\n",
      "|               THEFT|   22399|\n",
      "|             BATTERY|   19034|\n",
      "|     CRIMINAL DAMAGE|   11095|\n",
      "|             ASSAULT|    9302|\n",
      "|       OTHER OFFENSE|    7044|\n",
      "|  DECEPTIVE PRACTICE|    6369|\n",
      "| MOTOR VEHICLE THEFT|    5918|\n",
      "|   WEAPONS VIOLATION|    4122|\n",
      "|             ROBBERY|    3771|\n",
      "|            BURGLARY|    3262|\n",
      "|           NARCOTICS|    1999|\n",
      "|   CRIMINAL TRESPASS|    1913|\n",
      "|OFFENSE INVOLVING...|     913|\n",
      "|CRIMINAL SEXUAL A...|     684|\n",
      "|         SEX OFFENSE|     555|\n",
      "|PUBLIC PEACE VIOL...|     323|\n",
      "|            HOMICIDE|     301|\n",
      "|INTERFERENCE WITH...|     188|\n",
      "|               ARSON|     177|\n",
      "|            STALKING|     171|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crimeByTypeSql = spark.sql(\"\"\"\n",
    "                select\n",
    "                      wiki_changes.value.primary_type,\n",
    "                       count(*)\n",
    "                       \n",
    "                    from\n",
    "                        wiki_changes\n",
    "                        group by wiki_changes.value.primary_type\n",
    "                        order by count(*) desc\n",
    "                        \n",
    "\"\"\")\n",
    "crimeByTypeSql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite obvious that in our dataset, crimes like Theft and Battery are the most popular, 22.000 and 20.000 entries each. Top 3 is closing in Criminal damage offence with around 11.000 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the most popular location where crime was commited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using column \"location_description\" which stands for location where the crime was commited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/29 12:18:46 WARN TaskSetManager: Stage 29 contains a task of very large size (15094 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|location_description|count(1)|\n",
      "+--------------------+--------+\n",
      "|              STREET|   25651|\n",
      "|           APARTMENT|   21007|\n",
      "|           RESIDENCE|   13212|\n",
      "|            SIDEWALK|    4861|\n",
      "|PARKING LOT / GAR...|    3549|\n",
      "|  SMALL RETAIL STORE|    3212|\n",
      "|          RESTAURANT|    2115|\n",
      "|               ALLEY|    2025|\n",
      "|    DEPARTMENT STORE|    1632|\n",
      "|COMMERCIAL / BUSI...|    1511|\n",
      "|     OTHER (SPECIFY)|    1474|\n",
      "|         GAS STATION|    1463|\n",
      "|VEHICLE NON-COMME...|    1369|\n",
      "|RESIDENCE - PORCH...|    1341|\n",
      "|  RESIDENCE - GARAGE|    1142|\n",
      "|  GROCERY FOOD STORE|    1060|\n",
      "|SCHOOL - PUBLIC B...|    1017|\n",
      "|RESIDENCE - YARD ...|     906|\n",
      "|SCHOOL - PUBLIC G...|     759|\n",
      "|       BAR OR TAVERN|     687|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crimeByLocation = spark.sql(\"\"\"\n",
    "                select\n",
    "                      wiki_changes.value.location_description,\n",
    "                       count(*)\n",
    "                       \n",
    "                    from\n",
    "                        wiki_changes\n",
    "                        group by wiki_changes.value.location_description\n",
    "                        order by count(*) desc\n",
    "                        \n",
    "\"\"\")\n",
    "crimeByLocation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have our top 3 locations, Street, Apartment, Residence. Whilst crimes commited on the streets are common, apartment crimes are a different story. Lets find out wheather any of those crimes can be classified as Domestic violence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding out the rate of domestic violence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using column \"domestic\" which has values true/false and indicates whether the crime can be classifed as domestic violence.\n",
    "\n",
    "We are looking into the ratio for the crimes that happened inside the apartment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/29 12:27:13 WARN TaskSetManager: Stage 32 contains a task of very large size (15094 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|domestic|count(1)|\n",
      "+--------+--------+\n",
      "|   false|   11297|\n",
      "|    true|    9710|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crimeDomestic = spark.sql(\"\"\"\n",
    "                select\n",
    "                      wiki_changes.value.domestic,\n",
    "                       count(*)\n",
    "                       \n",
    "                    from\n",
    "                        wiki_changes\n",
    "                        where wiki_changes.value.location_description = \"APARTMENT\"\n",
    "                        group by wiki_changes.value.domestic\n",
    "                        order by count(*) desc\n",
    "                        \n",
    "\"\"\")\n",
    "crimeDomestic.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately half of the crimes that were reported in Apartments are considered a domestic violence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly see the whole picture, the domestic crime violence across all reported locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/29 12:33:48 WARN TaskSetManager: Stage 35 contains a task of very large size (15094 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|domestic|count(1)|\n",
      "+--------+--------+\n",
      "|   false|   80080|\n",
      "|    true|   19920|\n",
      "+--------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/29 13:14:53 WARN KafkaOffsetReaderConsumer: Error in attempt 1 getting Kafka offsets: \n",
      "org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition crimeData-chicago_100k-0 could be determined\n"
     ]
    }
   ],
   "source": [
    "crimesDomestilAll = spark.sql(\"\"\"\n",
    "                select\n",
    "                      wiki_changes.value.domestic,\n",
    "                       count(*)\n",
    "                       \n",
    "                    from\n",
    "                        wiki_changes\n",
    "                        group by wiki_changes.value.domestic\n",
    "                        order by count(*) desc\n",
    "                        \n",
    "\"\"\")\n",
    "crimesDomestilAll.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output we can see that around 20% of all reported crimes are domestically related, if put in comparison with crimes from within the apartments, it can be seen that domestic violense it prone to happen more often inside the house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving streamed data to a flat file and continuing out research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to save the unprocessed data into a flat file, namely a parquet type.\n",
    "\n",
    "We want to save the data before starting any data manipualtion in order to have a safe backup and the oppurtunity to share this file with other components of our project.\n",
    "\n",
    "Spark natively supports writing files to parquet. The only thing we need to do is to call function write and provide it with the new parquet file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/29 01:20:12 WARN TaskSetManager: Stage 3 contains a task of very large size (15094 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/06/29 01:20:44 ERROR Utils: Aborting task                        (0 + 4) / 4]\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:373)\n",
      "\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:396)\n",
      "\tat org.apache.parquet.io.api.Binary$ByteArrayBackedBinary.toByteBuffer(Binary.java:333)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:186)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:183)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator.compare(PrimitiveComparator.java:63)\n",
      "\tat org.apache.parquet.column.statistics.BinaryStatistics.updateStats(BinaryStatistics.java:60)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.write(ColumnWriterBase.java:240)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:476)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:206)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:204)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3456/579064316.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3501/1604491665.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3497/1342289083.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:468)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3488/357914748.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "22/06/29 01:20:47 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:373)\n",
      "\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:396)\n",
      "\tat org.apache.parquet.io.api.Binary$ByteArrayBackedBinary.toByteBuffer(Binary.java:333)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:186)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:183)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator.compare(PrimitiveComparator.java:63)\n",
      "\tat org.apache.parquet.column.statistics.BinaryStatistics.updateStats(BinaryStatistics.java:62)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.write(ColumnWriterBase.java:240)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:476)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:206)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:204)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3456/579064316.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3501/1604491665.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$17(ParquetWriteSupport.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3502/1577729524.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeGroup(ParquetWriteSupport.scala:474)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$16(ParquetWriteSupport.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$16$adapted(ParquetWriteSupport.scala:239)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3465/1209749393.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3501/1604491665.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3497/1342289083.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:468)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "22/06/29 01:20:51 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/06/29 01:20:55 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/06/29 01:20:58 WARN Utils: Suppressing exception in catch: Could not initialize class org.xerial.snappy.Snappy\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy\n",
      "\tat org.apache.parquet.hadoop.codec.SnappyCompressor.compress(SnappyCompressor.java:67)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n",
      "\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:167)\n",
      "\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:168)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.flush(ColumnWriteStoreBase.java:186)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:29)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:185)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:124)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:164)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:41)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:117)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:308)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/06/29 01:20:58 WARN Utils: Suppressing exception in catch: Could not initialize class org.xerial.snappy.Snappy\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy\n",
      "\tat org.apache.parquet.hadoop.codec.SnappyCompressor.compress(SnappyCompressor.java:67)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n",
      "\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:167)\n",
      "\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:168)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.flush(ColumnWriteStoreBase.java:186)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:29)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:185)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:124)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:164)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:41)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:117)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:308)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/06/29 01:20:58 WARN Utils: Suppressing exception in catch: Could not initialize class org.xerial.snappy.Snappy\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy\n",
      "\tat org.apache.parquet.hadoop.codec.SnappyCompressor.compress(SnappyCompressor.java:67)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n",
      "\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:167)\n",
      "\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:168)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.flush(ColumnWriteStoreBase.java:186)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:29)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:185)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:124)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:164)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:41)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:117)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:308)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/06/29 01:20:58 WARN Utils: Suppressing exception in catch: GC overhead limit exceeded\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/06/29 01:20:58 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 9)\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:373)\n",
      "\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:396)\n",
      "\tat org.apache.parquet.io.api.Binary$ByteArrayBackedBinary.toByteBuffer(Binary.java:333)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:186)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:183)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tat org.apache.parquet.schema.PrimitiveComparator.compare(PrimitiveComparator.java:63)\n",
      "\tat org.apache.parquet.column.statistics.BinaryStatistics.updateStats(BinaryStatistics.java:60)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.write(ColumnWriterBase.java:240)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:476)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:206)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:204)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3456/579064316.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3501/1604491665.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3497/1342289083.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:468)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3488/357914748.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "22/06/29 01:20:58 ERROR Executor: Exception in task 3.0 in stage 3.0 (TID 12)\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:373)\n",
      "\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:396)\n",
      "\tat org.apache.parquet.io.api.Binary$ByteArrayBackedBinary.toByteBuffer(Binary.java:333)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:186)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:183)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator.compare(PrimitiveComparator.java:63)\n",
      "\tat org.apache.parquet.column.statistics.BinaryStatistics.updateStats(BinaryStatistics.java:62)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.write(ColumnWriterBase.java:240)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:476)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:206)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:204)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3456/579064316.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3501/1604491665.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$17(ParquetWriteSupport.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3502/1577729524.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeGroup(ParquetWriteSupport.scala:474)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$16(ParquetWriteSupport.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$16$adapted(ParquetWriteSupport.scala:239)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3465/1209749393.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3501/1604491665.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3497/1342289083.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:468)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "22/06/29 01:20:58 ERROR Executor: Exception in task 1.0 in stage 3.0 (TID 10)\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/06/29 01:20:58 ERROR Executor: Exception in task 2.0 in stage 3.0 (TID 11)\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/06/29 01:20:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 3.0 (TID 11),5,main]\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/06/29 01:20:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 3.0 (TID 12),5,main]\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:373)\n",
      "\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:396)\n",
      "\tat org.apache.parquet.io.api.Binary$ByteArrayBackedBinary.toByteBuffer(Binary.java:333)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:186)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:183)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator.compare(PrimitiveComparator.java:63)\n",
      "\tat org.apache.parquet.column.statistics.BinaryStatistics.updateStats(BinaryStatistics.java:62)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.write(ColumnWriterBase.java:240)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:476)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:206)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:204)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3456/579064316.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3501/1604491665.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$17(ParquetWriteSupport.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3502/1577729524.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeGroup(ParquetWriteSupport.scala:474)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$16(ParquetWriteSupport.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$16$adapted(ParquetWriteSupport.scala:239)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3465/1209749393.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3501/1604491665.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3497/1342289083.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:468)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "22/06/29 01:20:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 3.0 (TID 10),5,main]\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/06/29 01:20:58 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 3.0 (TID 9),5,main]\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:373)\n",
      "\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:396)\n",
      "\tat org.apache.parquet.io.api.Binary$ByteArrayBackedBinary.toByteBuffer(Binary.java:333)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:186)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:183)\n",
      "\tat org.apache.parquet.schema.PrimitiveComparator.compare(PrimitiveComparator.java:63)\n",
      "\tat org.apache.parquet.column.statistics.BinaryStatistics.updateStats(BinaryStatistics.java:60)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.write(ColumnWriterBase.java:240)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:476)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:206)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:204)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3456/579064316.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3501/1604491665.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3497/1342289083.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:468)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3488/357914748.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "22/06/29 01:20:58 WARN TaskSetManager: Lost task 2.0 in stage 3.0 (TID 11) (192.168.0.101 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\n",
      "22/06/29 01:20:58 ERROR TaskSetManager: Task 2 in stage 3.0 failed 1 times; aborting job\n",
      "22/06/29 01:20:58 ERROR FileFormatWriter: Aborting job f48c2770-65a0-4e97-aab0-613a3f0377f4.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 1 times, most recent failure: Lost task 2.0 in stage 3.0 (TID 11) (192.168.0.101 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:781)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vasilii/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/vr/48qd0dvd3zx5h01r32l1_5k40000gn/T/ipykernel_62864/2232691093.py\", line 1, in <cell line: 1>\n",
      "    df.write.parquet(\"crimeData2.parquet\")\n",
      "  File \"/Users/vasilii/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py\", line 885, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/Users/vasilii/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/vasilii/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/vasilii/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vasilii/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vasilii/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/vasilii/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrimeData2.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py:885\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m--> 885\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:1993\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   1990\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   1991\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   1995\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py:542\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    536\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    537\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    539\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m'\u001b[39m : stb,\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    543\u001b[0m }\n\u001b[1;32m    545\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py:281\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py:288\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 288\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py:402\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"crimeData2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading out parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways we can read out a file of type parquet.\n",
    "\n",
    "1) Using pandas and pd.read_parquet() function, supplying path to the file itself and the parquet engine we are going to use (fastparquet in out case).\n",
    "\n",
    "2) Using Spark by creating a new sqlContext variable and using read.parquet() function.\n",
    "\n",
    "Both ways provide us with needed functionality:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_Crime_df= pd.read_parquet('crimeData.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_Crime_df = spark.createDataFrame(pandas_Crime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vasilii/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# using SQLContext to read parquet file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SQLContext\n\u001b[0;32m----> 3\u001b[0m sqlContext \u001b[38;5;241m=\u001b[39m \u001b[43mSQLContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# to read parquet file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m sqlContext\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrimeData.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/context.py:86\u001b[0m, in \u001b[0;36mSQLContext.__init__\u001b[0;34m(self, sparkContext, sparkSession, jsqlContext)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparkSession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     sparkSession \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jsqlContext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     jsqlContext \u001b[38;5;241m=\u001b[39m sparkSession\u001b[38;5;241m.\u001b[39m_jwrapped\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py:233\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 233\u001b[0m     \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msessionState\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconf()\u001b[38;5;241m.\u001b[39msetConfString(key, value)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py:281\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py:288\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 288\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py:402\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# using SQLContext to read parquet file\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# to read parquet file\n",
    "df = sqlContext.read.parquet('crimeData.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
